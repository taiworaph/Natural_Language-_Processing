{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/taiwoalabi/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers.core import Dense, Dropout, SpatialDropout1D\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.pooling import GlobalMaxPooling1D\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FILE = \"/Users/taiwoalabi/Downloads/winemag-data-130k-v2.csv\"\n",
    "TOKEN_GO = '<GO>'\n",
    "TOKEN_EOS = '<EOS>'\n",
    "TOKEN_PAD = '<PAD>'\n",
    "TOKEN_UNK = '<UNK>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Marie= pd.read_csv(INPUT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ii in range(len(Marie)):\n",
    "    Marie.loc[ii,\"Points1\"] = int(Marie.loc[ii,\"points\"])/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ii in range(len(Marie)):\n",
    "    if Marie.loc[ii,\"Points1\"] > 0.95:\n",
    "        Marie.loc[ii, \"Rating_Val\"]= \"AA\"\n",
    "    \n",
    "    elif (Marie.loc[ii, \"Points1\"] > 0.9) & (Marie.loc[ii, \"Points1\"] <= 0.95):\n",
    "        Marie.loc[ii, \"Rating_Val\"]= \"BB\"\n",
    "        \n",
    "    \n",
    "    elif (Marie.loc[ii, \"Points1\"] > 0.85) & (Marie.loc[ii, \"Points1\"] <= 0.9):\n",
    "        Marie.loc[ii, \"Rating_Val\"]= \"CC\"\n",
    "        \n",
    "    elif (Marie.loc[ii, \"Points1\"] <= 0.85):\n",
    "        Marie.loc[ii, \"Rating_Val\"]= \"DD\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Adjective_Nouns= ['JJ', 'JJR','JJS','NN', 'NNS', 'NNP', 'NNPS', 'VBN' ]\n",
    "for ii in range(len(Marie)):\n",
    "    Z= nltk.pos_tag(nltk.word_tokenize(Marie.loc[ii, \"description\"]))\n",
    "    ZZ= [ii[0] for ii in Z if ii[1] in Adjective_Nouns]\n",
    "    Marie.loc[ii,\"POS-Tag\"]= ' '.join(ZZ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence translation from this value ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ii in range(len(Marie)):\n",
    "    Marie.loc[ii,\"Machine_Trans1\"] = str(\"<GO>\") + \" \" + str(Marie.loc[ii, \"Rating_Val\"]) + \" \" + str(Marie.loc[ii,\"country\"]) + \" \" + str(Marie.loc[ii,\"province\"]) \\\n",
    "    + \" \" + str(Marie.loc[ii,\"variety\"]) + \".\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence translating to this value ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ii in range(len(Marie)):\n",
    "    Marie.loc[ii, \"description1\"] = str(Marie.loc[ii, \"description\"]) + \" \" + \"<EOS>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the Word Indice(s) and the Embedding Matrix -- Input Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = collections.Counter()\n",
    "maxlen = 0\n",
    "for iii in range(len(Marie)):\n",
    "    sentence = Marie['Machine_Trans1'][iii]\n",
    "    words = [x.lower() for x in nltk.word_tokenize(sentence)]\n",
    "    \n",
    "    if len(words) > maxlen:\n",
    "        maxlen = len(words)\n",
    "    for word in words:\n",
    "        counter[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z= word2index[\"<GO>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the maximum length of the Input Vocabulary sentences: 13\n"
     ]
    }
   ],
   "source": [
    "print(\"This is the maximum length of the Input Vocabulary sentences:\", maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 10000\n",
    "word2index = collections.defaultdict(int)\n",
    "for wid, word in enumerate(counter.most_common()):\n",
    "    word2index[word[0]] = wid + 1\n",
    "vocab_size = len(word2index) + 1\n",
    "index2word = {v:k for k, v in word2index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the vocabulary size of the entire Input-Vocabulary:  1166\n"
     ]
    }
   ],
   "source": [
    "print(\"This is the vocabulary size of the entire Input-Vocabulary: \", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### - VOCAB_SIZE must be - 1200\n",
    "# vocab_size = 1200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the Word Indice(s) and the Embedding Matrix -- Target Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = collections.Counter()\n",
    "maxlen = 0\n",
    "for iii in range(len(Marie)):\n",
    "    sentence = Marie['description1'][iii]\n",
    "    words = [x.lower() for x in nltk.word_tokenize(sentence)]\n",
    "    \n",
    "    if len(words) > maxlen:\n",
    "        maxlen = len(words)\n",
    "    for word in words:\n",
    "        counter[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the maximum length of the target Vocabulary sentences:  159\n"
     ]
    }
   ],
   "source": [
    "print(\"This is the maximum length of the target Vocabulary sentences: \", maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE2 = 5000\n",
    "word2index2 = collections.defaultdict(int)\n",
    "for wid, word in enumerate(counter.most_common()):\n",
    "    word2index2[word[0]] = wid + 1\n",
    "vocab_size2 = len(word2index2) + 1\n",
    "index2word2 = {v:k for k, v in word2index2.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the vocabulary size of the entire target-Vocabulary:  45967\n"
     ]
    }
   ],
   "source": [
    "print(\"This is the vocabulary size of the entire target-Vocabulary: \", vocab_size2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### - VOCAB_SIZE must be - 46000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs,ys = [], []\n",
    "\n",
    "for iii in range(len(Marie)):\n",
    "    sentence = Marie['Machine_Trans1'][iii]\n",
    "    sentence2 = Marie['description1'][iii]\n",
    "    words = [x.lower() for x in nltk.word_tokenize(sentence)]\n",
    "    wids = [word2index[word] for word in words]\n",
    "    \n",
    "    words2 = [x.lower() for x in nltk.word_tokenize(sentence2)]\n",
    "    wids2 = [word2index2[word] for word in words2]\n",
    "    xs.append(wids)\n",
    "    ys.append(wids2)\n",
    "\n",
    "X = pad_sequences(xs, maxlen=160) #X is the input i.e the wine rating, country, variety etc.\n",
    "Y = pad_sequences(ys, maxlen=160) #Y is the ouput i.e the description of the wine etc.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<GO> CC Italy Sicily & Sardinia White Blend.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Marie['Machine_Trans1'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  2,  3,  5,\n",
       "       10, 61, 60, 62, 29,  9,  4], dtype=int32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Aromas include tropical fruit, broom, brimstone and dried herb. The palate isn't overly expressive, offering unripened apple, citrus and dried sage alongside brisk acidity. <EOS>\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Marie['description1'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "          22,   854,   216,    20,     1,  1974,     1,  3604,     3,\n",
       "          92,   108,     2,     4,    23,    12,   200,  1007,   910,\n",
       "           1,   308, 16368,    55,     1,    67,     3,    92,   421,\n",
       "         137,   360,    24,     2,     8,     7,     9], dtype=int32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain_id, Xtest_id, Ytrain_id, Ytest_id = train_test_split(X, Y, test_size=0.3, random_state=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "XtrainReal = Xtrain_id[1:6000]\n",
    "YtrainReal = Ytrain_id[1:6000]\n",
    "\n",
    "XtestReal = Xtest_id[6000:10000]\n",
    "YtestReal = Ytest_id[6000:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_true = X[1:20000]\n",
    "Y_true = Y[1:20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test= X[0:101]\n",
    "Y_test=Y[0:101]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59999"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Machine Translation without the Attention mechanism ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import json, os, re, shutil, sys, time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(42)\n",
    "\n",
    "Src_Vocab_Size = 1200\n",
    "embedding_size = 500\n",
    "num_units = 500\n",
    "learning_rate = 0.001\n",
    "\n",
    "trg_Vocab_Size = 46000\n",
    "embedding_size2 = 500\n",
    "\n",
    "#Max encoder time vs batch size\n",
    "encoder_inputs = tf.placeholder(tf.int32, [None, None], name = \"Encoder_input\")\n",
    "\n",
    "#Max decoder time vs batch size\n",
    "decoder_inputs = tf.placeholder(tf.int32, [None, None], name = \"Decoder_input\")\n",
    "\n",
    "#Decoder output - These are the decoder output shifted to the left by one time step with an end-of-sentence tag appended to the right - max_decoder_time vs batch_size\n",
    "decoder_outputs = tf.placeholder(tf.int32, [None, None], name = \"Decoder_output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/taiwoalabi/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "#Getting the batch-size\n",
    "with tf.name_scope(\"Encode_batch_size\"):\n",
    "    batch_size = tf.shape(encoder_inputs)[1]\n",
    "    \n",
    "with tf.name_scope(\"Decoder_max_time\"):\n",
    "    max_time_decode= tf.shape(decoder_inputs)[0]\n",
    "    \n",
    "with tf.name_scope(\"Decode_batch_size\"):\n",
    "    batch_size_decode = tf.shape(decoder_inputs)[1]\n",
    "\n",
    "#Getting the max-time\n",
    "with tf.name_scope(\"Encode_max_time\"):\n",
    "    max_time = tf.shape(encoder_inputs)[0]\n",
    "    \n",
    "    ns_ = tf.tile([max_time],[batch_size,], name = \"ns\")\n",
    "    ns_2 = tf.tile([max_time_decode],[batch_size_decode,], name = \"ns2\")\n",
    "\n",
    "with tf.name_scope(\"Embedding_Layer_Encoder\"):\n",
    "    embedding_encoder = tf.Variable(tf.random_uniform([Src_Vocab_Size,embedding_size], -1.0, 1.0), name=\"Embed_Encoder\")\n",
    "    encoder_emb_inp = tf.nn.embedding_lookup(embedding_encoder,encoder_inputs)\n",
    "\n",
    "with tf.name_scope(\"Embedding_Layer_Decoder\"):\n",
    "    embedding_decoder = tf.Variable(tf.random_uniform([trg_Vocab_Size,embedding_size2], -1.0, 1.0), name=\"Embed_decoder\")\n",
    "    decoder_emb_inp = tf.nn.embedding_lookup(embedding_decoder,decoder_inputs)\n",
    "    \n",
    "\n",
    "with tf.name_scope(\"Encoder_RNN_Cell\"):\n",
    "\n",
    "    \n",
    "    encoder_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units = num_units)\n",
    "    #initial_h_ = encoder_cell.zero_state(batch_size, dtype = tf.float32)\n",
    "    encoder_outputs, encoder_state = tf.nn.dynamic_rnn(encoder_cell, encoder_emb_inp, sequence_length = ns_, time_major = True, dtype= tf.float32)\n",
    "    \n",
    "    \n",
    "with tf.name_scope(\"Decode_LSTM_Cell\"):\n",
    "    projection_layer = tf.layers.Dense(trg_Vocab_Size, use_bias = False)\n",
    "    decoder_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units)\n",
    "    helper = tf.contrib.seq2seq.TrainingHelper(decoder_emb_inp, ns_2, time_major = True)\n",
    "    decoder = tf.contrib.seq2seq.BasicDecoder(decoder_cell, helper, encoder_state, output_layer = projection_layer)\n",
    "\n",
    "with tf.name_scope(\"Dynamic_Decoding\"):\n",
    "    outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder)\n",
    "    logits = outputs.rnn_output\n",
    "    \n",
    "with tf.name_scope(\"Cost_Function\"):\n",
    "    crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = decoder_outputs, logits = logits)\n",
    "    train_loss = tf.reduce_mean(crossent, name = \"sparse_softmax\")\n",
    "    \n",
    "with tf.name_scope(\"Gradient_Optimization\"):\n",
    "    params = tf.trainable_variables()\n",
    "    gradients = tf.gradients(train_loss, params)\n",
    "    clipped_gradients, _ = tf.clip_by_global_norm(gradients, 1)\n",
    "    \n",
    "    alpha_ = tf.placeholder(tf.float32, name=\"learning_rate\")\n",
    "    optimizer = tf.train.AdamOptimizer(alpha_)\n",
    "    update_step = optimizer.apply_gradients(zip(clipped_gradients,params))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializer step\n",
    "init_ = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_writer = tf.summary.FileWriter(\"~/Desktop/Machine_Learning_Projects\", \n",
    "                                       tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_batch_generator(batch_size, *data_arrays):\n",
    "    \"\"\"Generate minibatches from multiple columns of data.\n",
    "\n",
    "    Example:\n",
    "        for (bx, by) in multi_batch_generator(5, x, y):\n",
    "            # bx is minibatch for x\n",
    "            # by is minibatch for y\n",
    "\n",
    "    Args:\n",
    "      batch_size: int, batch size\n",
    "      data_arrays: one or more array-like, supporting slicing along the first\n",
    "        dimension, and with matching first dimension.\n",
    "\n",
    "    Yields:\n",
    "      minibatches of maximum size batch_size\n",
    "    \"\"\"\n",
    "    assert(data_arrays)\n",
    "    num_examples = len(data_arrays[0])\n",
    "    for i in range(1, len(data_arrays)):\n",
    "        assert(len(data_arrays[i]) == num_examples)\n",
    "\n",
    "    for i in range(0, num_examples, batch_size):\n",
    "        # Yield matching slices from each data array.\n",
    "        yield tuple(data[i:i+batch_size] for data in data_arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def run_epoch(session, batch_iterator,\n",
    "              train=False, verbose=False,\n",
    "              tick_s=10, learning_rate=None):\n",
    "    assert(learning_rate is not None)\n",
    "    start_time = time.time()\n",
    "    tick_time = start_time  # for showing status\n",
    "    total_cost = 0.0  # total cost, summed over all words\n",
    "    total_batches = 0\n",
    "    total_words = 0\n",
    "\n",
    "    if train:\n",
    "        train_op = update_step\n",
    "        \n",
    "        loss = train_loss\n",
    "    else:\n",
    "        train_op = tf.no_op()\n",
    "        use_dropout = False  # no dropout at test time\n",
    "        loss = train_loss  # true loss, if train_loss is an approximation\n",
    "\n",
    "    for i, (x, y) in enumerate(batch_iterator):\n",
    "        cost = 0.0\n",
    "        XX= np.transpose(y)\n",
    "        #print(XX)\n",
    "        #print(x)\n",
    "        #print(y)\n",
    "        \n",
    "        feed_dict = {encoder_inputs:x, decoder_inputs:y,alpha_:learning_rate,decoder_outputs:XX}\n",
    "        _, cost = session.run([train_op, loss],\n",
    "                       feed_dict=feed_dict)\n",
    "        \n",
    "\n",
    "        #### END(YOUR CODE) ####\n",
    "        total_cost += cost\n",
    "        total_batches = i + 1\n",
    "        total_words += x.size  # w.size = batch_size * max_time\n",
    "\n",
    "        ##\n",
    "        # Print average loss-so-far for epoch\n",
    "        # If using train_loss_, this may be an underestimate.\n",
    "        if verbose and (time.time() - tick_time >= tick_s):\n",
    "            avg_cost = total_cost / total_batches\n",
    "            avg_wps = total_words / (time.time() - start_time)\n",
    "            print(\"[batch {:d}]: seen {:d} words at {:.1f} wps, loss = {:.3f}\".format(\n",
    "                i, total_words, avg_wps, avg_cost))\n",
    "            tick_time = time.time()  # reset time ticker\n",
    "\n",
    "    return total_cost / total_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_dataset(session, X, Y, name=\"Data\"):\n",
    "    # For scoring, we can use larger batches to speed things up.\n",
    "    batch_size =200\n",
    "    bi = multi_batch_generator(batch_size, X, Y)\n",
    "    cost = run_epoch(session, bi, \n",
    "                     learning_rate=0.01, train=False, \n",
    "                     verbose=False, tick_s=3600)\n",
    "    print(\"{:s}: avg. loss: {:.03f}  (perplexity: {:.02f})\".format(name, cost, np.exp(cost)))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_timedelta(fmt=\"%d:%02d:%02d\", since=None, until=None):\n",
    "    \"\"\"Pretty-print a timedelta, using the given format string.\"\"\"\n",
    "    since = since or time.time()\n",
    "    until = until or time.time()\n",
    "    delta_s = until - since\n",
    "    hours, remainder = divmod(delta_s, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    return fmt % (hours, minutes, seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_SAVEDIR = \"Desktop/Machine_Learning_Projects/\"\n",
    "checkpoint_filename = os.path.join(TF_SAVEDIR, \"NMT\")\n",
    "trained_filename = os.path.join(TF_SAVEDIR, \"NMT_trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[epoch 1] Starting epoch 1\n",
      "[batch 0]: seen 32000 words at 96.8 wps, loss = 10.727\n",
      "[batch 1]: seen 64000 words at 100.0 wps, loss = 8.754\n",
      "[batch 2]: seen 96000 words at 100.8 wps, loss = 7.096\n",
      "[batch 3]: seen 128000 words at 102.2 wps, loss = 5.998\n",
      "[batch 4]: seen 160000 words at 103.2 wps, loss = 5.307\n",
      "[batch 5]: seen 192000 words at 103.9 wps, loss = 4.869\n",
      "[batch 6]: seen 224000 words at 104.0 wps, loss = 4.531\n",
      "[batch 7]: seen 256000 words at 104.7 wps, loss = 4.293\n",
      "[batch 8]: seen 288000 words at 104.5 wps, loss = 4.069\n",
      "[batch 9]: seen 320000 words at 104.4 wps, loss = 3.879\n",
      "[batch 10]: seen 352000 words at 105.4 wps, loss = 3.709\n",
      "[batch 11]: seen 384000 words at 106.4 wps, loss = 3.564\n",
      "[batch 12]: seen 416000 words at 107.3 wps, loss = 3.449\n",
      "[batch 13]: seen 448000 words at 107.5 wps, loss = 3.328\n",
      "[batch 14]: seen 480000 words at 107.2 wps, loss = 3.229\n",
      "[batch 15]: seen 512000 words at 107.2 wps, loss = 3.146\n",
      "[batch 16]: seen 544000 words at 108.1 wps, loss = 3.067\n",
      "[batch 17]: seen 576000 words at 109.0 wps, loss = 2.989\n",
      "[batch 18]: seen 608000 words at 109.5 wps, loss = 2.914\n",
      "[batch 19]: seen 640000 words at 110.1 wps, loss = 2.856\n",
      "[batch 20]: seen 672000 words at 110.7 wps, loss = 2.800\n",
      "[batch 21]: seen 704000 words at 110.5 wps, loss = 2.743\n",
      "[batch 22]: seen 736000 words at 110.7 wps, loss = 2.690\n",
      "[batch 23]: seen 768000 words at 110.5 wps, loss = 2.647\n",
      "[batch 24]: seen 800000 words at 110.2 wps, loss = 2.601\n",
      "[batch 25]: seen 832000 words at 110.3 wps, loss = 2.564\n",
      "[batch 26]: seen 864000 words at 110.2 wps, loss = 2.518\n",
      "[batch 27]: seen 896000 words at 110.0 wps, loss = 2.476\n",
      "[batch 28]: seen 928000 words at 110.3 wps, loss = 2.439\n",
      "[batch 29]: seen 960000 words at 110.2 wps, loss = 2.391\n",
      "[batch 30]: seen 992000 words at 110.3 wps, loss = 2.354\n",
      "[batch 31]: seen 1024000 words at 110.2 wps, loss = 2.316\n",
      "[batch 32]: seen 1056000 words at 110.3 wps, loss = 2.276\n",
      "[batch 33]: seen 1088000 words at 110.2 wps, loss = 2.239\n",
      "[batch 34]: seen 1120000 words at 110.0 wps, loss = 2.201\n",
      "[batch 35]: seen 1152000 words at 109.9 wps, loss = 2.162\n",
      "[batch 36]: seen 1184000 words at 109.8 wps, loss = 2.123\n",
      "[batch 37]: seen 1216000 words at 109.8 wps, loss = 2.084\n",
      "[batch 38]: seen 1248000 words at 109.7 wps, loss = 2.046\n",
      "[batch 39]: seen 1280000 words at 109.8 wps, loss = 2.010\n",
      "[batch 40]: seen 1312000 words at 109.7 wps, loss = 1.973\n",
      "[batch 41]: seen 1344000 words at 109.8 wps, loss = 1.939\n",
      "[batch 42]: seen 1376000 words at 109.8 wps, loss = 1.905\n",
      "[batch 43]: seen 1408000 words at 110.0 wps, loss = 1.871\n",
      "[batch 44]: seen 1440000 words at 110.1 wps, loss = 1.840\n",
      "[batch 45]: seen 1472000 words at 110.1 wps, loss = 1.809\n",
      "[batch 46]: seen 1504000 words at 110.2 wps, loss = 1.779\n",
      "[batch 47]: seen 1536000 words at 110.4 wps, loss = 1.750\n",
      "[batch 48]: seen 1568000 words at 110.6 wps, loss = 1.721\n",
      "[batch 49]: seen 1600000 words at 110.5 wps, loss = 1.694\n",
      "[batch 50]: seen 1632000 words at 110.6 wps, loss = 1.667\n",
      "[batch 51]: seen 1664000 words at 110.5 wps, loss = 1.640\n",
      "[batch 52]: seen 1696000 words at 110.5 wps, loss = 1.614\n",
      "[batch 53]: seen 1728000 words at 110.4 wps, loss = 1.589\n",
      "[batch 54]: seen 1760000 words at 110.5 wps, loss = 1.565\n",
      "[batch 55]: seen 1792000 words at 110.6 wps, loss = 1.541\n",
      "[batch 56]: seen 1824000 words at 110.6 wps, loss = 1.517\n",
      "[batch 57]: seen 1856000 words at 110.9 wps, loss = 1.495\n",
      "[batch 58]: seen 1888000 words at 111.1 wps, loss = 1.473\n",
      "[batch 59]: seen 1920000 words at 111.3 wps, loss = 1.451\n",
      "[batch 60]: seen 1952000 words at 111.5 wps, loss = 1.429\n",
      "[batch 61]: seen 1984000 words at 111.7 wps, loss = 1.410\n",
      "[batch 62]: seen 2016000 words at 111.7 wps, loss = 1.389\n",
      "[batch 63]: seen 2048000 words at 111.7 wps, loss = 1.370\n",
      "[batch 64]: seen 2080000 words at 111.6 wps, loss = 1.352\n",
      "[batch 65]: seen 2112000 words at 111.7 wps, loss = 1.333\n",
      "[batch 66]: seen 2144000 words at 111.6 wps, loss = 1.316\n",
      "[batch 67]: seen 2176000 words at 111.7 wps, loss = 1.298\n",
      "[batch 68]: seen 2208000 words at 111.6 wps, loss = 1.281\n",
      "[batch 69]: seen 2240000 words at 111.6 wps, loss = 1.265\n",
      "[batch 70]: seen 2272000 words at 111.6 wps, loss = 1.249\n",
      "[batch 71]: seen 2304000 words at 111.6 wps, loss = 1.233\n",
      "[batch 72]: seen 2336000 words at 111.6 wps, loss = 1.218\n",
      "[batch 73]: seen 2368000 words at 111.5 wps, loss = 1.203\n",
      "[batch 74]: seen 2400000 words at 111.5 wps, loss = 1.189\n",
      "[batch 75]: seen 2432000 words at 111.5 wps, loss = 1.175\n",
      "[batch 76]: seen 2464000 words at 111.4 wps, loss = 1.161\n",
      "[batch 77]: seen 2496000 words at 111.5 wps, loss = 1.147\n",
      "[batch 78]: seen 2528000 words at 111.4 wps, loss = 1.135\n",
      "[batch 79]: seen 2560000 words at 111.4 wps, loss = 1.122\n",
      "[batch 80]: seen 2592000 words at 111.4 wps, loss = 1.109\n",
      "[batch 81]: seen 2624000 words at 111.5 wps, loss = 1.097\n",
      "[batch 82]: seen 2656000 words at 111.6 wps, loss = 1.085\n",
      "[batch 83]: seen 2688000 words at 111.6 wps, loss = 1.073\n",
      "[batch 84]: seen 2720000 words at 111.5 wps, loss = 1.062\n",
      "[batch 85]: seen 2752000 words at 111.3 wps, loss = 1.050\n",
      "[batch 86]: seen 2784000 words at 111.0 wps, loss = 1.039\n",
      "[batch 87]: seen 2816000 words at 110.7 wps, loss = 1.028\n",
      "[batch 88]: seen 2848000 words at 110.6 wps, loss = 1.018\n",
      "[batch 89]: seen 2880000 words at 110.5 wps, loss = 1.007\n",
      "[batch 90]: seen 2912000 words at 110.6 wps, loss = 0.997\n",
      "[batch 91]: seen 2944000 words at 110.5 wps, loss = 0.987\n",
      "[batch 92]: seen 2976000 words at 110.5 wps, loss = 0.977\n",
      "[batch 93]: seen 3008000 words at 110.5 wps, loss = 0.968\n",
      "[batch 94]: seen 3040000 words at 110.5 wps, loss = 0.958\n",
      "[batch 95]: seen 3072000 words at 110.5 wps, loss = 0.949\n",
      "[batch 96]: seen 3104000 words at 110.5 wps, loss = 0.940\n",
      "[batch 97]: seen 3136000 words at 110.6 wps, loss = 0.931\n",
      "[batch 98]: seen 3168000 words at 110.7 wps, loss = 0.923\n",
      "[batch 99]: seen 3199840 words at 110.9 wps, loss = 0.914\n",
      "[epoch 1] Completed in 8:01:00\n",
      "[epoch 1] Train set: avg. loss: 0.066  (perplexity: 1.07)\n",
      "[epoch 1] Test set: avg. loss: 0.001  (perplexity: 1.00)\n",
      "\n",
      "[epoch 2] Starting epoch 2\n",
      "[batch 0]: seen 32000 words at 100.1 wps, loss = 0.061\n",
      "[batch 1]: seen 64000 words at 100.3 wps, loss = 0.070\n",
      "[batch 2]: seen 96000 words at 101.9 wps, loss = 0.072\n",
      "[batch 3]: seen 128000 words at 103.3 wps, loss = 0.071\n",
      "[batch 4]: seen 160000 words at 104.4 wps, loss = 0.067\n",
      "[batch 5]: seen 192000 words at 105.9 wps, loss = 0.067\n",
      "[batch 6]: seen 224000 words at 105.7 wps, loss = 0.066\n",
      "[batch 7]: seen 256000 words at 105.7 wps, loss = 0.067\n",
      "[batch 8]: seen 288000 words at 106.6 wps, loss = 0.066\n",
      "[batch 9]: seen 320000 words at 106.5 wps, loss = 0.064\n",
      "[batch 10]: seen 352000 words at 106.8 wps, loss = 0.063\n",
      "[batch 11]: seen 384000 words at 107.3 wps, loss = 0.062\n",
      "[batch 12]: seen 416000 words at 107.3 wps, loss = 0.061\n",
      "[batch 13]: seen 448000 words at 107.3 wps, loss = 0.060\n",
      "[batch 14]: seen 480000 words at 108.0 wps, loss = 0.059\n",
      "[batch 15]: seen 512000 words at 107.7 wps, loss = 0.059\n",
      "[batch 16]: seen 544000 words at 107.6 wps, loss = 0.060\n",
      "[batch 17]: seen 576000 words at 107.4 wps, loss = 0.059\n",
      "[batch 18]: seen 608000 words at 108.2 wps, loss = 0.058\n",
      "[batch 19]: seen 640000 words at 108.9 wps, loss = 0.058\n",
      "[batch 20]: seen 672000 words at 109.4 wps, loss = 0.057\n",
      "[batch 21]: seen 704000 words at 109.9 wps, loss = 0.057\n",
      "[batch 22]: seen 736000 words at 110.1 wps, loss = 0.056\n",
      "[batch 23]: seen 768000 words at 110.0 wps, loss = 0.056\n",
      "[batch 24]: seen 800000 words at 109.8 wps, loss = 0.056\n",
      "[batch 25]: seen 832000 words at 109.6 wps, loss = 0.056\n",
      "[batch 26]: seen 864000 words at 109.7 wps, loss = 0.056\n",
      "[batch 27]: seen 896000 words at 109.5 wps, loss = 0.055\n",
      "[batch 28]: seen 928000 words at 109.2 wps, loss = 0.055\n",
      "[batch 29]: seen 960000 words at 109.0 wps, loss = 0.054\n",
      "[batch 30]: seen 992000 words at 109.2 wps, loss = 0.054\n",
      "[batch 31]: seen 1024000 words at 109.5 wps, loss = 0.053\n",
      "[batch 32]: seen 1056000 words at 109.7 wps, loss = 0.053\n",
      "[batch 33]: seen 1088000 words at 109.6 wps, loss = 0.053\n",
      "[batch 34]: seen 1120000 words at 109.1 wps, loss = 0.053\n",
      "[batch 35]: seen 1152000 words at 108.9 wps, loss = 0.052\n",
      "[batch 36]: seen 1184000 words at 108.7 wps, loss = 0.052\n",
      "[batch 37]: seen 1216000 words at 108.6 wps, loss = 0.051\n",
      "[batch 38]: seen 1248000 words at 108.5 wps, loss = 0.051\n",
      "[batch 39]: seen 1280000 words at 108.5 wps, loss = 0.050\n",
      "[batch 40]: seen 1312000 words at 108.5 wps, loss = 0.050\n",
      "[batch 41]: seen 1344000 words at 108.6 wps, loss = 0.049\n",
      "[batch 42]: seen 1376000 words at 108.4 wps, loss = 0.049\n",
      "[batch 43]: seen 1408000 words at 108.5 wps, loss = 0.049\n",
      "[batch 44]: seen 1440000 words at 108.5 wps, loss = 0.048\n",
      "[batch 45]: seen 1472000 words at 108.5 wps, loss = 0.048\n",
      "[batch 46]: seen 1504000 words at 108.5 wps, loss = 0.048\n",
      "[batch 47]: seen 1536000 words at 108.6 wps, loss = 0.047\n",
      "[batch 48]: seen 1568000 words at 108.6 wps, loss = 0.047\n",
      "[batch 49]: seen 1600000 words at 108.5 wps, loss = 0.047\n",
      "[batch 50]: seen 1632000 words at 108.5 wps, loss = 0.046\n",
      "[batch 51]: seen 1664000 words at 108.6 wps, loss = 0.046\n",
      "[batch 52]: seen 1696000 words at 108.5 wps, loss = 0.045\n",
      "[batch 53]: seen 1728000 words at 108.5 wps, loss = 0.045\n",
      "[batch 54]: seen 1760000 words at 108.6 wps, loss = 0.044\n",
      "[batch 55]: seen 1792000 words at 108.7 wps, loss = 0.044\n",
      "[batch 56]: seen 1824000 words at 108.6 wps, loss = 0.043\n",
      "[batch 57]: seen 1856000 words at 108.7 wps, loss = 0.043\n",
      "[batch 58]: seen 1888000 words at 108.7 wps, loss = 0.043\n",
      "[batch 59]: seen 1920000 words at 109.0 wps, loss = 0.042\n",
      "[batch 60]: seen 1952000 words at 109.3 wps, loss = 0.042\n",
      "[batch 61]: seen 1984000 words at 109.5 wps, loss = 0.041\n",
      "[batch 62]: seen 2016000 words at 109.7 wps, loss = 0.041\n",
      "[batch 63]: seen 2048000 words at 110.0 wps, loss = 0.041\n",
      "[batch 64]: seen 2080000 words at 110.1 wps, loss = 0.040\n",
      "[batch 65]: seen 2112000 words at 110.3 wps, loss = 0.040\n",
      "[batch 66]: seen 2144000 words at 110.4 wps, loss = 0.039\n",
      "[batch 67]: seen 2176000 words at 110.6 wps, loss = 0.039\n",
      "[batch 68]: seen 2208000 words at 110.8 wps, loss = 0.039\n",
      "[batch 69]: seen 2240000 words at 111.0 wps, loss = 0.038\n",
      "[batch 70]: seen 2272000 words at 111.5 wps, loss = 0.038\n",
      "[batch 71]: seen 2304000 words at 112.1 wps, loss = 0.038\n",
      "[batch 72]: seen 2336000 words at 112.8 wps, loss = 0.037\n",
      "[batch 73]: seen 2368000 words at 113.4 wps, loss = 0.037\n",
      "[batch 74]: seen 2400000 words at 114.0 wps, loss = 0.037\n",
      "[batch 75]: seen 2432000 words at 114.6 wps, loss = 0.036\n",
      "[batch 76]: seen 2464000 words at 115.2 wps, loss = 0.036\n",
      "[batch 77]: seen 2496000 words at 115.9 wps, loss = 0.036\n",
      "[batch 78]: seen 2528000 words at 116.5 wps, loss = 0.036\n",
      "[batch 79]: seen 2560000 words at 117.1 wps, loss = 0.035\n",
      "[batch 80]: seen 2592000 words at 117.6 wps, loss = 0.035\n",
      "[batch 81]: seen 2624000 words at 118.2 wps, loss = 0.035\n",
      "[batch 82]: seen 2656000 words at 118.8 wps, loss = 0.034\n",
      "[batch 83]: seen 2688000 words at 119.3 wps, loss = 0.034\n",
      "[batch 84]: seen 2720000 words at 119.9 wps, loss = 0.034\n",
      "[batch 85]: seen 2752000 words at 120.4 wps, loss = 0.034\n",
      "[batch 86]: seen 2784000 words at 120.8 wps, loss = 0.033\n",
      "[batch 87]: seen 2816000 words at 121.3 wps, loss = 0.033\n",
      "[batch 88]: seen 2848000 words at 121.7 wps, loss = 0.033\n",
      "[batch 89]: seen 2880000 words at 122.1 wps, loss = 0.033\n",
      "[batch 90]: seen 2912000 words at 122.6 wps, loss = 0.032\n",
      "[batch 91]: seen 2944000 words at 123.1 wps, loss = 0.032\n",
      "[batch 92]: seen 2976000 words at 123.6 wps, loss = 0.032\n",
      "[batch 93]: seen 3008000 words at 124.1 wps, loss = 0.031\n",
      "[batch 94]: seen 3040000 words at 124.6 wps, loss = 0.031\n",
      "[batch 95]: seen 3072000 words at 125.0 wps, loss = 0.031\n",
      "[batch 96]: seen 3104000 words at 125.5 wps, loss = 0.031\n",
      "[batch 97]: seen 3136000 words at 126.0 wps, loss = 0.031\n",
      "[batch 98]: seen 3168000 words at 126.4 wps, loss = 0.030\n",
      "[batch 99]: seen 3199840 words at 126.8 wps, loss = 0.030\n",
      "[epoch 2] Completed in 7:00:26\n",
      "[epoch 2] Train set: avg. loss: 0.038  (perplexity: 1.04)\n",
      "[epoch 2] Test set: avg. loss: 0.001  (perplexity: 1.00)\n",
      "\n",
      "[epoch 3] Starting epoch 3\n",
      "[batch 0]: seen 32000 words at 344.4 wps, loss = 0.006\n",
      "[batch 1]: seen 64000 words at 333.0 wps, loss = 0.009\n",
      "[batch 2]: seen 96000 words at 328.0 wps, loss = 0.010\n",
      "[batch 3]: seen 128000 words at 326.6 wps, loss = 0.010\n",
      "[batch 4]: seen 160000 words at 327.6 wps, loss = 0.009\n",
      "[batch 5]: seen 192000 words at 326.6 wps, loss = 0.010\n",
      "[batch 6]: seen 224000 words at 327.3 wps, loss = 0.010\n",
      "[batch 7]: seen 256000 words at 327.7 wps, loss = 0.010\n",
      "[batch 8]: seen 288000 words at 329.3 wps, loss = 0.010\n",
      "[batch 9]: seen 320000 words at 328.0 wps, loss = 0.010\n",
      "[batch 10]: seen 352000 words at 327.8 wps, loss = 0.009\n",
      "[batch 11]: seen 384000 words at 328.9 wps, loss = 0.009\n",
      "[batch 12]: seen 416000 words at 329.4 wps, loss = 0.009\n",
      "[batch 13]: seen 448000 words at 330.2 wps, loss = 0.009\n",
      "[batch 14]: seen 480000 words at 330.8 wps, loss = 0.009\n",
      "[batch 15]: seen 512000 words at 331.2 wps, loss = 0.009\n",
      "[batch 16]: seen 544000 words at 331.7 wps, loss = 0.009\n",
      "[batch 17]: seen 576000 words at 331.7 wps, loss = 0.009\n",
      "[batch 18]: seen 608000 words at 331.9 wps, loss = 0.009\n",
      "[batch 19]: seen 640000 words at 332.1 wps, loss = 0.009\n",
      "[batch 20]: seen 672000 words at 331.9 wps, loss = 0.009\n",
      "[batch 21]: seen 704000 words at 331.7 wps, loss = 0.009\n",
      "[batch 22]: seen 736000 words at 331.4 wps, loss = 0.009\n",
      "[batch 23]: seen 768000 words at 331.5 wps, loss = 0.009\n",
      "[batch 24]: seen 800000 words at 324.4 wps, loss = 0.009\n",
      "[batch 25]: seen 832000 words at 315.9 wps, loss = 0.009\n",
      "[batch 26]: seen 864000 words at 308.9 wps, loss = 0.009\n",
      "[batch 27]: seen 896000 words at 303.1 wps, loss = 0.008\n",
      "[batch 28]: seen 928000 words at 296.2 wps, loss = 0.008\n",
      "[batch 29]: seen 960000 words at 290.7 wps, loss = 0.008\n",
      "[batch 30]: seen 992000 words at 285.4 wps, loss = 0.008\n",
      "[batch 31]: seen 1024000 words at 281.8 wps, loss = 0.008\n",
      "[batch 32]: seen 1056000 words at 278.3 wps, loss = 0.008\n",
      "[batch 33]: seen 1088000 words at 274.9 wps, loss = 0.008\n",
      "[batch 34]: seen 1120000 words at 271.7 wps, loss = 0.008\n",
      "[batch 35]: seen 1152000 words at 269.1 wps, loss = 0.008\n",
      "[batch 36]: seen 1184000 words at 266.7 wps, loss = 0.008\n",
      "[batch 37]: seen 1216000 words at 264.2 wps, loss = 0.008\n",
      "[batch 38]: seen 1248000 words at 261.7 wps, loss = 0.008\n",
      "[batch 39]: seen 1280000 words at 259.9 wps, loss = 0.008\n",
      "[batch 40]: seen 1312000 words at 257.9 wps, loss = 0.008\n",
      "[batch 41]: seen 1344000 words at 256.3 wps, loss = 0.007\n",
      "[batch 42]: seen 1376000 words at 254.3 wps, loss = 0.007\n",
      "[batch 43]: seen 1408000 words at 253.0 wps, loss = 0.007\n",
      "[batch 44]: seen 1440000 words at 251.4 wps, loss = 0.007\n",
      "[batch 45]: seen 1472000 words at 249.8 wps, loss = 0.007\n",
      "[batch 46]: seen 1504000 words at 248.2 wps, loss = 0.007\n",
      "[batch 47]: seen 1536000 words at 246.7 wps, loss = 0.007\n",
      "[batch 48]: seen 1568000 words at 245.1 wps, loss = 0.007\n",
      "[batch 49]: seen 1600000 words at 244.0 wps, loss = 0.007\n",
      "[batch 50]: seen 1632000 words at 242.9 wps, loss = 0.007\n",
      "[batch 51]: seen 1664000 words at 242.0 wps, loss = 0.007\n",
      "[batch 52]: seen 1696000 words at 241.0 wps, loss = 0.007\n",
      "[batch 53]: seen 1728000 words at 240.0 wps, loss = 0.006\n",
      "[batch 54]: seen 1760000 words at 239.1 wps, loss = 0.006\n",
      "[batch 55]: seen 1792000 words at 238.2 wps, loss = 0.006\n",
      "[batch 56]: seen 1824000 words at 237.6 wps, loss = 0.006\n",
      "[batch 57]: seen 1856000 words at 236.9 wps, loss = 0.006\n",
      "[batch 58]: seen 1888000 words at 236.5 wps, loss = 0.006\n",
      "[batch 59]: seen 1920000 words at 236.1 wps, loss = 0.006\n",
      "[batch 60]: seen 1952000 words at 235.8 wps, loss = 0.006\n",
      "[batch 61]: seen 1984000 words at 235.5 wps, loss = 0.006\n",
      "[batch 62]: seen 2016000 words at 235.1 wps, loss = 0.006\n",
      "[batch 63]: seen 2048000 words at 234.4 wps, loss = 0.006\n",
      "[batch 64]: seen 2080000 words at 233.6 wps, loss = 0.006\n",
      "[batch 65]: seen 2112000 words at 233.0 wps, loss = 0.006\n",
      "[batch 66]: seen 2144000 words at 232.5 wps, loss = 0.005\n",
      "[batch 67]: seen 2176000 words at 232.0 wps, loss = 0.005\n",
      "[batch 68]: seen 2208000 words at 231.5 wps, loss = 0.005\n",
      "[batch 69]: seen 2240000 words at 230.8 wps, loss = 0.005\n",
      "[batch 70]: seen 2272000 words at 230.4 wps, loss = 0.005\n",
      "[batch 71]: seen 2304000 words at 229.9 wps, loss = 0.005\n",
      "[batch 72]: seen 2336000 words at 229.5 wps, loss = 0.005\n",
      "[batch 73]: seen 2368000 words at 229.1 wps, loss = 0.005\n",
      "[batch 74]: seen 2400000 words at 228.7 wps, loss = 0.005\n",
      "[batch 75]: seen 2432000 words at 227.9 wps, loss = 0.005\n",
      "[batch 76]: seen 2464000 words at 227.5 wps, loss = 0.005\n",
      "[batch 77]: seen 2496000 words at 227.1 wps, loss = 0.005\n",
      "[batch 78]: seen 2528000 words at 226.7 wps, loss = 0.005\n",
      "[batch 79]: seen 2560000 words at 226.4 wps, loss = 0.005\n",
      "[batch 80]: seen 2592000 words at 226.0 wps, loss = 0.005\n",
      "[batch 81]: seen 2624000 words at 225.6 wps, loss = 0.005\n",
      "[batch 82]: seen 2656000 words at 225.3 wps, loss = 0.005\n",
      "[batch 83]: seen 2688000 words at 224.7 wps, loss = 0.005\n",
      "[batch 84]: seen 2720000 words at 224.4 wps, loss = 0.004\n",
      "[batch 85]: seen 2752000 words at 224.0 wps, loss = 0.004\n",
      "[batch 86]: seen 2784000 words at 223.7 wps, loss = 0.004\n",
      "[batch 87]: seen 2816000 words at 223.4 wps, loss = 0.004\n",
      "[batch 88]: seen 2848000 words at 222.8 wps, loss = 0.004\n",
      "[batch 89]: seen 2880000 words at 222.5 wps, loss = 0.004\n",
      "[batch 90]: seen 2912000 words at 222.0 wps, loss = 0.004\n",
      "[batch 91]: seen 2944000 words at 221.7 wps, loss = 0.004\n",
      "[batch 92]: seen 2976000 words at 221.4 wps, loss = 0.004\n",
      "[batch 93]: seen 3008000 words at 220.9 wps, loss = 0.004\n",
      "[batch 94]: seen 3040000 words at 220.6 wps, loss = 0.004\n",
      "[batch 95]: seen 3072000 words at 220.4 wps, loss = 0.004\n",
      "[batch 96]: seen 3104000 words at 220.3 wps, loss = 0.004\n",
      "[batch 97]: seen 3136000 words at 220.2 wps, loss = 0.004\n",
      "[batch 98]: seen 3168000 words at 220.1 wps, loss = 0.004\n",
      "[batch 99]: seen 3199840 words at 220.1 wps, loss = 0.004\n",
      "[epoch 3] Completed in 4:02:19\n",
      "[epoch 3] Train set: avg. loss: 0.035  (perplexity: 1.04)\n",
      "[epoch 3] Test set: avg. loss: 0.000  (perplexity: 1.00)\n",
      "\n",
      "[epoch 4] Starting epoch 4\n",
      "[batch 0]: seen 32000 words at 182.0 wps, loss = 0.000\n",
      "[batch 1]: seen 64000 words at 190.2 wps, loss = 0.000\n",
      "[batch 2]: seen 96000 words at 192.3 wps, loss = 0.000\n",
      "[batch 3]: seen 128000 words at 193.7 wps, loss = 0.000\n",
      "[batch 4]: seen 160000 words at 195.9 wps, loss = 0.000\n",
      "[batch 5]: seen 192000 words at 188.7 wps, loss = 0.000\n",
      "[batch 6]: seen 224000 words at 188.3 wps, loss = 0.000\n",
      "[batch 7]: seen 256000 words at 186.4 wps, loss = 0.000\n",
      "[batch 8]: seen 288000 words at 188.0 wps, loss = 0.000\n",
      "[batch 9]: seen 320000 words at 187.6 wps, loss = 0.000\n",
      "[batch 10]: seen 352000 words at 189.0 wps, loss = 0.000\n",
      "[batch 11]: seen 384000 words at 190.1 wps, loss = 0.000\n",
      "[batch 12]: seen 416000 words at 190.6 wps, loss = 0.000\n",
      "[batch 13]: seen 448000 words at 191.3 wps, loss = 0.000\n",
      "[batch 14]: seen 480000 words at 192.1 wps, loss = 0.000\n",
      "[batch 15]: seen 512000 words at 193.3 wps, loss = 0.000\n",
      "[batch 16]: seen 544000 words at 194.4 wps, loss = 0.000\n",
      "[batch 17]: seen 576000 words at 195.6 wps, loss = 0.000\n",
      "[batch 18]: seen 608000 words at 196.5 wps, loss = 0.000\n",
      "[batch 19]: seen 640000 words at 197.1 wps, loss = 0.000\n",
      "[batch 20]: seen 672000 words at 197.1 wps, loss = 0.000\n",
      "[batch 21]: seen 704000 words at 197.4 wps, loss = 0.000\n",
      "[batch 22]: seen 736000 words at 197.2 wps, loss = 0.000\n",
      "[batch 23]: seen 768000 words at 197.1 wps, loss = 0.000\n",
      "[batch 24]: seen 800000 words at 197.3 wps, loss = 0.000\n",
      "[batch 25]: seen 832000 words at 196.8 wps, loss = 0.000\n",
      "[batch 26]: seen 864000 words at 196.3 wps, loss = 0.000\n",
      "[batch 27]: seen 896000 words at 196.4 wps, loss = 0.000\n",
      "[batch 28]: seen 928000 words at 196.6 wps, loss = 0.000\n",
      "[batch 29]: seen 960000 words at 196.6 wps, loss = 0.000\n",
      "[batch 30]: seen 992000 words at 195.4 wps, loss = 0.000\n",
      "[batch 31]: seen 1024000 words at 194.9 wps, loss = 0.000\n",
      "[batch 32]: seen 1056000 words at 193.8 wps, loss = 0.000\n",
      "[batch 33]: seen 1088000 words at 192.4 wps, loss = 0.000\n",
      "[batch 34]: seen 1120000 words at 190.8 wps, loss = 0.000\n",
      "[batch 35]: seen 1152000 words at 189.1 wps, loss = 0.000\n",
      "[batch 36]: seen 1184000 words at 187.8 wps, loss = 0.000\n",
      "[batch 37]: seen 1216000 words at 186.5 wps, loss = 0.000\n",
      "[batch 38]: seen 1248000 words at 185.2 wps, loss = 0.000\n",
      "[batch 39]: seen 1280000 words at 184.2 wps, loss = 0.000\n",
      "[batch 40]: seen 1312000 words at 183.7 wps, loss = 0.000\n",
      "[batch 41]: seen 1344000 words at 183.1 wps, loss = 0.000\n",
      "[batch 42]: seen 1376000 words at 182.3 wps, loss = 0.000\n",
      "[batch 43]: seen 1408000 words at 181.6 wps, loss = 0.000\n",
      "[batch 44]: seen 1440000 words at 181.8 wps, loss = 0.000\n",
      "[batch 45]: seen 1472000 words at 181.6 wps, loss = 0.000\n",
      "[batch 46]: seen 1504000 words at 181.4 wps, loss = 0.000\n",
      "[batch 47]: seen 1536000 words at 181.7 wps, loss = 0.000\n",
      "[batch 48]: seen 1568000 words at 181.6 wps, loss = 0.000\n",
      "[batch 49]: seen 1600000 words at 181.8 wps, loss = 0.000\n",
      "[batch 50]: seen 1632000 words at 182.2 wps, loss = 0.000\n",
      "[batch 51]: seen 1664000 words at 182.6 wps, loss = 0.000\n",
      "[batch 52]: seen 1696000 words at 183.0 wps, loss = 0.000\n",
      "[batch 53]: seen 1728000 words at 183.3 wps, loss = 0.000\n",
      "[batch 54]: seen 1760000 words at 183.3 wps, loss = 0.000\n",
      "[batch 55]: seen 1792000 words at 182.6 wps, loss = 0.000\n",
      "[batch 56]: seen 1824000 words at 182.4 wps, loss = 0.000\n",
      "[batch 57]: seen 1856000 words at 182.0 wps, loss = 0.000\n",
      "[batch 58]: seen 1888000 words at 181.9 wps, loss = 0.000\n",
      "[batch 59]: seen 1920000 words at 181.6 wps, loss = 0.000\n",
      "[batch 60]: seen 1952000 words at 181.2 wps, loss = 0.000\n",
      "[batch 61]: seen 1984000 words at 180.5 wps, loss = 0.000\n",
      "[batch 62]: seen 2016000 words at 180.0 wps, loss = 0.000\n",
      "[batch 63]: seen 2048000 words at 179.7 wps, loss = 0.000\n",
      "[batch 64]: seen 2080000 words at 179.3 wps, loss = 0.000\n",
      "[batch 65]: seen 2112000 words at 178.9 wps, loss = 0.000\n",
      "[batch 66]: seen 2144000 words at 178.5 wps, loss = 0.000\n",
      "[batch 67]: seen 2176000 words at 178.2 wps, loss = 0.000\n",
      "[batch 68]: seen 2208000 words at 178.1 wps, loss = 0.000\n",
      "[batch 69]: seen 2240000 words at 177.6 wps, loss = 0.000\n",
      "[batch 70]: seen 2272000 words at 177.4 wps, loss = 0.000\n",
      "[batch 71]: seen 2304000 words at 177.2 wps, loss = 0.000\n",
      "[batch 72]: seen 2336000 words at 176.6 wps, loss = 0.000\n",
      "[batch 73]: seen 2368000 words at 176.3 wps, loss = 0.000\n",
      "[batch 74]: seen 2400000 words at 175.8 wps, loss = 0.000\n",
      "[batch 75]: seen 2432000 words at 175.4 wps, loss = 0.000\n",
      "[batch 76]: seen 2464000 words at 175.1 wps, loss = 0.000\n",
      "[batch 77]: seen 2496000 words at 174.8 wps, loss = 0.000\n",
      "[batch 78]: seen 2528000 words at 174.7 wps, loss = 0.000\n",
      "[batch 79]: seen 2560000 words at 174.3 wps, loss = 0.000\n",
      "[batch 80]: seen 2592000 words at 173.9 wps, loss = 0.000\n",
      "[batch 81]: seen 2624000 words at 173.5 wps, loss = 0.000\n",
      "[batch 82]: seen 2656000 words at 173.0 wps, loss = 0.000\n",
      "[batch 83]: seen 2688000 words at 172.6 wps, loss = 0.000\n",
      "[batch 84]: seen 2720000 words at 172.5 wps, loss = 0.000\n",
      "[batch 85]: seen 2752000 words at 172.7 wps, loss = 0.000\n",
      "[batch 86]: seen 2784000 words at 172.8 wps, loss = 0.000\n",
      "[batch 87]: seen 2816000 words at 172.9 wps, loss = 0.000\n",
      "[batch 88]: seen 2848000 words at 172.8 wps, loss = 0.000\n",
      "[batch 89]: seen 2880000 words at 172.5 wps, loss = 0.000\n",
      "[batch 90]: seen 2912000 words at 172.1 wps, loss = 0.000\n",
      "[batch 91]: seen 2944000 words at 171.6 wps, loss = 0.000\n",
      "[batch 92]: seen 2976000 words at 171.3 wps, loss = 0.000\n",
      "[batch 93]: seen 3008000 words at 171.0 wps, loss = 0.000\n",
      "[batch 94]: seen 3040000 words at 170.9 wps, loss = 0.000\n",
      "[batch 95]: seen 3072000 words at 170.4 wps, loss = 0.000\n",
      "[batch 96]: seen 3104000 words at 170.1 wps, loss = 0.000\n",
      "[batch 97]: seen 3136000 words at 169.9 wps, loss = 0.000\n",
      "[batch 98]: seen 3168000 words at 169.6 wps, loss = 0.000\n",
      "[batch 99]: seen 3199840 words at 169.3 wps, loss = 0.000\n",
      "[epoch 4] Completed in 5:15:03\n",
      "[epoch 4] Train set: avg. loss: 0.035  (perplexity: 1.04)\n",
      "[epoch 4] Test set: avg. loss: 0.000  (perplexity: 1.00)\n",
      "\n",
      "[epoch 5] Starting epoch 5\n",
      "[batch 0]: seen 32000 words at 198.1 wps, loss = 0.000\n",
      "[batch 1]: seen 64000 words at 191.7 wps, loss = 0.000\n",
      "[batch 2]: seen 96000 words at 188.8 wps, loss = 0.000\n",
      "[batch 3]: seen 128000 words at 182.6 wps, loss = 0.000\n",
      "[batch 4]: seen 160000 words at 174.5 wps, loss = 0.000\n",
      "[batch 5]: seen 192000 words at 169.8 wps, loss = 0.000\n",
      "[batch 6]: seen 224000 words at 168.3 wps, loss = 0.000\n",
      "[batch 7]: seen 256000 words at 168.1 wps, loss = 0.000\n",
      "[batch 8]: seen 288000 words at 165.0 wps, loss = 0.000\n",
      "[batch 9]: seen 320000 words at 165.8 wps, loss = 0.000\n",
      "[batch 10]: seen 352000 words at 167.8 wps, loss = 0.000\n",
      "[batch 11]: seen 384000 words at 167.9 wps, loss = 0.000\n",
      "[batch 12]: seen 416000 words at 166.1 wps, loss = 0.000\n",
      "[batch 13]: seen 448000 words at 166.2 wps, loss = 0.000\n",
      "[batch 14]: seen 480000 words at 166.0 wps, loss = 0.000\n",
      "[batch 15]: seen 512000 words at 166.3 wps, loss = 0.000\n",
      "[batch 16]: seen 544000 words at 166.2 wps, loss = 0.000\n",
      "[batch 17]: seen 576000 words at 164.1 wps, loss = 0.000\n",
      "[batch 18]: seen 608000 words at 162.6 wps, loss = 0.000\n",
      "[batch 19]: seen 640000 words at 160.6 wps, loss = 0.000\n",
      "[batch 20]: seen 672000 words at 154.2 wps, loss = 0.000\n",
      "[batch 21]: seen 704000 words at 148.4 wps, loss = 0.000\n",
      "[batch 22]: seen 736000 words at 144.7 wps, loss = 0.000\n",
      "[batch 23]: seen 768000 words at 141.4 wps, loss = 0.000\n",
      "[batch 24]: seen 800000 words at 139.0 wps, loss = 0.000\n",
      "[batch 25]: seen 832000 words at 136.3 wps, loss = 0.000\n",
      "[batch 26]: seen 864000 words at 134.2 wps, loss = 0.000\n",
      "[batch 27]: seen 896000 words at 132.1 wps, loss = 0.000\n",
      "[batch 28]: seen 928000 words at 129.8 wps, loss = 0.000\n",
      "[batch 29]: seen 960000 words at 128.2 wps, loss = 0.000\n",
      "[batch 30]: seen 992000 words at 126.9 wps, loss = 0.000\n",
      "[batch 31]: seen 1024000 words at 125.5 wps, loss = 0.000\n",
      "[batch 32]: seen 1056000 words at 124.5 wps, loss = 0.000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-c1daac4ba84d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m               \u001b[0;31m#tick_s=10, learning_rate=None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# Run a training epoch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtick_s\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m#### END(YOUR CODE) ####\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-e67a8eac4f2d>\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(session, batch_iterator, train, verbose, tick_s, learning_rate)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mencoder_inputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_inputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0malpha_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdecoder_outputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mXX\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         _, cost = session.run([train_op, loss],\n\u001b[0;32m---> 30\u001b[0;31m                        feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1312\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "batch_size = 200\n",
    "learning_rate = 0.01  \n",
    "print_every = 1000\n",
    "saver = tf.train.Saver()\n",
    "np.random.seed(42)\n",
    "\n",
    "session = tf.Session()\n",
    "session.run(init_)\n",
    "\n",
    "t0 = time.time()\n",
    "for epoch in range(1,num_epochs+1):\n",
    "    t0_epoch = time.time()\n",
    "    epoch_cost = 0.0\n",
    "    total_batches = 0\n",
    "    print (\"\")\n",
    "    for epoch in range(1,num_epochs+1):\n",
    "        t0_epoch = time.time()\n",
    "        bi = multi_batch_generator(batch_size, X_true, Y_true)\n",
    "        print(\"[epoch {:d}] Starting epoch {:d}\".format(epoch, epoch))\n",
    "        #### YOUR CODE HERE ####\n",
    "        #(lm, session, batch_iterator,\n",
    "              #train=False, verbose=False,\n",
    "              #tick_s=10, learning_rate=None)\n",
    "        # Run a training epoch.\n",
    "        run_epoch(session, bi, train=True, tick_s=10,learning_rate =learning_rate, verbose= True)\n",
    "        \n",
    "        #### END(YOUR CODE) ####\n",
    "        print(\"[epoch {:d}] Completed in {:s}\".format(epoch, pretty_timedelta(since=t0_epoch)))\n",
    "    \n",
    "        # Save a checkpoint\n",
    "        saver.save(session, checkpoint_filename, global_step=epoch)\n",
    "    \n",
    "        ##\n",
    "        # score_dataset will run a forward pass over the entire dataset\n",
    "        # and report perplexity scores. This can be slow (around 1/2 to \n",
    "        # 1/4 as long as a full epoch), so you may want to comment it out\n",
    "        # to speed up training on a slow machine. Be sure to run it at the \n",
    "        # end to evaluate your score.\n",
    "        print(\"[epoch {:d}]\".format(epoch), end=\" \")\n",
    "        score_dataset(session, XtrainReal, YtrainReal, name=\"Train set\")\n",
    "        print(\"[epoch {:d}]\".format(epoch), end=\" \")\n",
    "        score_dataset(session, XtestReal, XtestReal, name=\"Test set\")\n",
    "        print(\"\")\n",
    "    \n",
    "    # Save final model\n",
    "    saver.save(session, trained_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is the inference step for decoding ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embedding_decoder, tf.fill([batch_size],9),894)\n",
    "\n",
    "decoder = tf.contrib.seq2seq.BasicDecoder(decoder_cell, helper, encoder_state, output_layer=projection_layer)\n",
    "maximum_iterations =tf.round(tf.reduce_max(ns_)*2)\n",
    "\n",
    "outputs, _, _= tf.contrib.seq2seq.dynamic_decode(decoder)\n",
    "translations =outputs.sample_id\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
