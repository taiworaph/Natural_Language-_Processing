## Abstract

> Sentiment analysis is one of the several areas of intense research in Natural Language Processing. The goal is to be able to map the summary meanings within a sentence or amongst a group of sentences to a label or group of labels. We investigate in this group project a sentiment analysis approach to classifying Sommelier sentiments on wines from various regions around the world and mapping these sentiments to a continuous numerical variable (wine score/wine grade). For sentiment analysis and mapping to a continuous domain, we investigated several state-of-the-art neural networks including: convolutional neural networks (CNN), long-short term memory (LSTM) neural networks and gated recurrent unit (GRU) neural network. These neural networks were evaluated against each other for computational performance and perplexity scores. This project also explored using POS tags to extract features of most significant relevance to the eventual tag. These features include Sommelier comments with either a noun and/or adjective part of speech. These enriched wording were then explored for use as word-embeddings to feed into the various neural networks described above. Results in the discussion section also show how loss and mean-absolute error can be improved in a GRU via changing the number of epochs, number of hidden units and word-embedding vector size. 
> We were able to successfully match Sommelier sentiments to ratings using all 3 investigated neural networks. GRU was found to be more computationally efficient with a mean absolute error value of 0.0244.



## Results and Discussion

#### Data-Preprocessing:
> The percentage points (with range from 80-100%) were normalized by dividing by 100 to give a range between 0.8 and 1. Normalization amongst other benefits also ensures that we do not see the exploding gradient issue when running LSTMs.
In addition, because the word-embedding space of the Sommelier descriptions is ripe with words that carry non-essential meanings like articles, determiners, punctuations etc., we explored using a  sentiment space that was reformatted. Each Sommelier comment was passed through a POS tag algorithm (NLTK pos-tagger). The words with POS tags having ‘JJ’, ‘JJR’, ‘JJS’, ‘NN’, ‘NNS’, ‘NNP’, ‘NNPS’, and ‘VBN.’ were used as a new feature space and served as input to the neural architectures previously shown above. The graph below shows the comparison between the loss and mean-absolute error from a GRU architecture fed with raw Sommelier comments vs a GRU architecture fed with POS tagged words carrying a richer embed matrix.
 
> The loss and mean-absolute error values for both models do not appear significantly different, however there appears to be a very slight improvement in the initial loss function score for the pre-formatted comments using POS tags. This initial gain in loss and mean absolute error is carried across through the entire training and is reflected in the overall loss and mae values at the end of the training. The mean absolute error and raw-different for GRU trained with raw Sommelier comments are  0.0245 and -0.0037 on the validation dataset vs 0.0244 and -0.0034 on GRU trained with POS tagged words.
All the other model results discussed below were evaluated using a word-embedding space pre-formatted with POS tags.
Data pre-processing for the seq-2-seq model was done for both the decoder and the encoder input matrix. The input to the encoder consisted of the a <GO> pre-tag, percentage points reformatted to a scale of tags between AA (>0.95), BB (0.9<BB<=0.95), CC (0.85<CC<=0.9) and DD (<=0.85); country, province and wine variety. 
The input to the decoder LSTM consists of word embeddings from the raw Sommelier description accompanied with an <EOS> tag. Both input to the Encoder and Decoder LSTMs were padded to the same length before they were fed to the embedding layer. 
Tuning of two hyper-parameters were investigated on this project- The number of epochs and the learning rate of the SGD optimizer. The number of epochs of training was varied from 5 epochs to 10 epochs for each model. As evident in the results section, 5 epoch(s) are not sufficient enough for training the GRU and the LSTM model since the MAE as well as loss are still been optimized. However, for the CNN algorithm training for 5 epochs vs training for a longer period does not show a very significant change in the MAE or loss. 
> To ensure uniformity across all training models we decided to stay with a training epoch of 10 for all models investigated.
We went ahead with using a learning rate of 0.001 across all architectures primarily because the CNN network appeared to already be overfitting at the current learning rate of 0.001. Although it does appear going with a learning rate of 0.01, as shown below for the bi-LSTM, may have better.
 
#### Results:
> From the results shown below (note the y-axis of all 3 graphs are not on the same scale), the CNN algorithm appears to perform worse on the validation dataset when compared to bidirectional LSTM and the GRU neural architectures. On the CNN architecture there was no regularization used although dropout was used to help prevent overfitting. The GRU architecture, however, also did not use regularization but the overall performance on the validation data outperforms that of the CNN and slightly better than the LSTM algorithm. 
It is interesting to note that loss for a bi-LSTM starts initially at a high value but upon training drops much faster and uniformly on both the training and the validation dataset than it does on the GRU and the bi-directional LSTM. The GRU architecture also seems to perform slightly better than the CNN and the bi-LSTM algorithm in terms of the loss. 
 
> It should be noted that loss and mean-absolute-error decay over time on both the GRU and bi-LSTM is indicative that both algorithms will benefit with more training and will potentially outperform the CNN algorithm in mean-absolute error as well as loss. The GRU algorithm, in terms of train time, is also superior to both the bi-LSTM and the CNN as it takes a relatively shorter train time. This is expected considering that there are more trainable parameters in the CNN network- 10.7 million (1st layer convolution)- compared to 1.5million (1st layer GRU) trainable parameters on the GRU architecture. 
Of the 3 models investigated, the GRU is the model for the same train period, batch-size and epoch; the GRU delivers on low perplexity scores and low mean absolute error. After training the GRU for 4 iterations a few lines of codes were written to extract features that have the maximum deviation between the absolute value of the (y_pred – y_test). 
 
> When the sentences above are compared against sentences that have lower absolute error values it does appear that the more the wine is described in a storyline format (meaning described in comparison to something else) the more confused the algorithm gets in trying to predict the final rating. To eliminate such confusion I think datasets with similar ‘synonym-like’ rating will need to be fed to the GRU algorithm.
A seq-2-seq translator algorithm was also investigated for use in mapping wine rating, source, variety to a predicted Sommelier comment space. Training of the seq-2-seq generative network was done on only a subset(20k) of the 120k dataset because training takes an immensely long time. After training the network for 4 epochs perplexity as low as 1.04 was obtained on the trained dataset and a perplexity of 1 was obtained on the test dataset. It should be noted that these perplexity values are likely to change when the algorithm is exposed to the full dataset of 120k. The addition of an attention algorithm that is state-of-the-art for sequence-2-sequence translation is also expected to significantly improve model accuracy when the algorithm is eventually exposed to the full dataset.

